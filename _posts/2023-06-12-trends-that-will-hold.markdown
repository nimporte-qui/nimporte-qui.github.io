---
layout: post
title:  "trends that will win"
date:   2023-06-12 00:00:00 -0400
categories: ai
---

We gather the summaries of deep learning from the previous sections and, in conjunction with the in-depth understanding of its mechanics and implications, we reproject it into future paradigms.

Large, deep neural networks compress the world into its parameters. Though this was not necessarily clear half a decade ago – when the prevailing understanding was that they were stochastic parrots that modeled statistical distributions of their training data – it is now clear that they conceptualize at a higher level of abstraction that comes with modeling p(x). Though their level of compression and understanding of the world may be shocking to many, it is more factual than intuitive. Unlike traditional statistical methods such as Bayesian inference, which were theoretically elegant solutions but could not be configured to represent a good solution, [a neural network that is deep and large enough can represent the solution](https://en.wikipedia.org/wiki/Universal_approximation_theorem). Configuring this good solution requires a large dataset and computation to do the optimization.

We now analyze the interesting compressive properties of deep neural networks, specifically large language models. How is it that simple next-token prediction with billions of neurons over petaflops of text data can encode shades of culture, idioms, and ideologies. It turns out that even though next-token prediction is a simple task, it requires increasing levels of linguistic understanding to be done correctly. Imagine that we are given a novel of a murder mystery and at the very end of the plot, the detective says: "I have figured out who the murderer is, and her name is ____." Though there are 171k words in the English language (and 14.7M in GPT-3’s vocabulary due to subword tokenization), it requires a complex of linguistic reasoning and memory to be able to predict the next word correctly. Inevitably, the process of learning statistical correlations over words requires compression.

Naturally, the preference of language over other modalities may raise some questions, as we clearly perceive the world through multiple senses. If we really think about it, language is the interface of humans – though there are other modalities of expression (namely visual and audial), there is nothing that preserves as much meaning as language. Chances are that our thoughts actually exist in some higher dimensional abstraction, but language is the way we project that down into something concrete and common with others. And of course, cultural hues are embedded in our thoughts, which translates to the specific languages that end up representing them.

This, however, does not dismiss the role of other modalities in intelligence. We might note that language models have never "seen" before, yet still understand things visually (through a textual medium, of course). For example, LLMs understand how blue is more similar to purple than it is to orange – perhaps scientifically – despite never having seen color before (we can also make the human analogy to blindness). We might further argue that should LLMs tie language to vision, it might learn this association without having to read billions of pages of how the sky and water are both blue to understand what blue is. From this conjecture, we hypothesize that there must be an exchange rate between modalities that dictates the learning curve of conceptual understanding. After all, the average human probably hears and sees no more than the order of magnitude of 1B words in our entire lifetime.

At this point, it still might not be immediately clear how this understanding of AI dictates how things will trend, and more importantly, how to take actionable steps. Below is the outline.

The (remaining?) glaring issue is the inability to continually learn, introspect, and adapt efficiently and autonomously to new compressions of the world. As humans, we change our chemical composition (neuroplasticity) – or source code – as we continue to learn about the world and also draw on prior intuition to adapt to new situations. However, there is an incubation process that modulates the learning process throughout our lives. For example, in childhood, we learn from ground truth through explicit labeling of our environment through textbooks, teachers, and parents. Gradually, we build our own representations of the world that grow in complexity and abstraction – we learn arithmetic, which allows us to learn algebra and geometry and calculus, which allow us to intuit aspects of the world that transcend our physical senses. However, after some point, we grow by introspecting on our own experiences and become even more complex and mature. In the same way, at some adolescent level of proficiency, neural networks can begin to introspect and learn from themselves, which is evident in projects like AutoGPT and babyagi.

We want to create intelligence that is personally and flexibly adaptive, but not an algorithm. An algorithm implies a system that runs and optimizes autonomously, but intelligence can interface with its underlying algorithms. Currently, we can use LLMs to index databases (i.e. LangChain), but we would be able to use AGI to index the world, a strict superset of information available as text (i.e. GPT-4’s consumption of vision, which is more proficient than operating on language alone). We would be able to tell AGI to learn more about a certain topic (i.e. babyagi, ChatGPT browser, Perplexity), and it will autonomously set off and do its homework before returning with a new compression of the relevant topics. The key is that the algorithm doesn’t blindly overfit absolutely, it should have the autonomy to do so.

One day, we will have our own custom AGI, much like the smartphone and the PC. Perhaps this has already started with ChatGPT and its ability to retain context in multiple sessions. Its adaptation does not occur at the source code level — it is frozen but able to be resourceful and augment its weight space. This would be analogous to a human whose ability to learn is frozen and they would not be able to learn anything new without explicit access to it. Rather, humans learn by editing their own source code and this seems like a property that AGI should be defined to have. I should note that the self-update loop – and all things related to AI capabilities – requires human alignment for a couple reasons. For practical reasons, all products must be human-centric, and AI that is unaligned with human preference will ultimately be utilitarian suboptimal. For existential reasons, it should be pretty clear why this must be a requirement.

I think there are a few themes here that [Ilya Sutskever has echoed in some of his interviews](https://www.youtube.com/watch?v=XjSUJUL9ADw&t=446s&ab_channel=XiaoYang). There is a future where AGI will not be a model whose source code is common for everyone and inflexible throughout time without manual update by the third-party owner. In theory, there is a spectrum of decentralization of AGI (monopoly, oligarchy, democracy) but irrefutably, the nature of intelligence alone can not be monopolized, but the execution of intelligence creation and ownership can be consolidated (?).

### open-source

[Google: "We have no moat, and neither does OpenAI"](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither?utm_source=tldrai)

Open-source models are terrible but this gap is shortening rapidly and will inevitably close. One could attribute this to many reasons, but it is undeniably a testament to the nature of software development. Since the last open-source version of GPT(-2), we have witnessed the rapid improvement of open-source LLMs through [GPT-J (2021)](https://www.eleuther.ai/artifacts/gpt-j), [GPT-Neo (2022)](https://www.eleuther.ai/artifacts/gpt-neo), [OPT (2022)](https://arxiv.org/abs/2205.01068), [BLOOM (2022)](https://arxiv.org/abs/2211.05100), [LLaMA (2023)](https://arxiv.org/abs/2302.13971), [StableLM (2023)](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models), [Pythia (2023)](https://arxiv.org/abs/2304.01373), and their respective ecosystems. Of course, the nature of open-source is inherently limited in model, dataset, compute scale — and perhaps more importantly as we are seeing with ChatGPT and RLHF — the necessity of high quality human data to maximize "preference" (as opposed to just "accuracy").

We should not think of these limitations as prohibitive in the absolute sense. In addition to supplementing foundation models with other senses to bootstrap the learning process (next section), we should think of (open-source) foundation models as maturing along some growth curve not too dissimilar to that of humans. For example, we must learn from teacher supervision to begin to understand what Shakespearean literature is, but after a certain level of proficiency, we are able to learn autonomously and introspectively (next next section) — properties that arguably represent the hallmark of human intelligence better than pure performative capability.

Existing diagrams of the software stack categorize companies as either at or above the foundation model layer, but open-source makes it possible to exist in between "at" and "above." Think about it as such: there must exist some base layer of general intelligence (i.e. high school level proficiency) but at higher education and professional specializations, intelligence as a stack becomes more vertically integrated and there becomes an increasing need for specificity. We not only want intelligent experts, but also the method in creating that expertise. Expertise is not necessarily stagnant; in the context of a whole field, expertise is fluid and evolves autonomously with time.

### multi-modality

The shortcut to data/compute/scale would be to bootstrap other modalities to supplement the learning of language. After all, humans only see/hear 1B tokens in our whole lives, but ground that learning in other modalities rich in information i.e. vision. For example, though it is easier to understand what colors are with access to visual senses than it is if one were blind (is this true?), a blind person’s concept of color is not handicapped.

Along this thread would be an analogy to how we learn through video. A documentary could arguably be half as effective should it only be audio. Audio paired with video enriches our senses multiplicatively. This makes sense, as "concepts" transcend what is measurable by a single sense. I return to the analogy of a concept existing in a high-dimensional abstract space and is projected down to lower-dimensional spaces of our senses. Of course, not all components are created equal, and some senses are lossier than others.

This idea of multi-modality is not necessarily novel — in a way, we can view domain-specific data as a sort of modality (pseudo-modality, I suppose) in which there is a distinction in the content that the data carries and its effect on the model (sharper learning curve towards salient information present in the curation). However, unlike these pseudo-modalities, a true modality separate from text would inevitably induce a different set of scaling laws. The 2D axes that we normally plot pareto curves on would have an extra dimension (z-axis) that reflects the effect of multimodal training. Of course, this can be further adjusted to take into account multimodal data curation.

### self-learning

While the first two subsections in this section have been more about the ways in which bringing up custom intelligence can be hacked / accelerated, I think this subsection introduces the critical component of scalable adaptability.

Arguably one of the most distinguishing features about human intelligence that sets it apart from intelligence of other mammals is conscious reasoning. There is something metaphysical about our intelligence that allows us to become aware of our capabilities and voluntarily learn from them — the source code is neuroplastic enough to edit itself. Of course, self-learning (introspection) in its fullest form does not occur uniformly amongst all instances of human intelligence. It is a custom process that happens at some point along the trajectory of intellectual development, but also to varying degrees in varying aspects depending on external stimuli.

Self-learning feels like the (pen)ultimate pillar to the problem of AI. In reality, learning does not happen in a vacuum. Our own learning processes are shaped by everything around us — social constructs breed us to align with the rest of society. In this sense, we must provide AI with the correct constraints in their self-learning process.
