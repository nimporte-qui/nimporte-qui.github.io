---
layout: post
title:  "deep learning"
date:   2023-01-16 00:00:00 -0400
categories: ai
---
Despite its breathtaking accomplishments in the recent weeks, months, and years, AI is hardly a newfound plaything of creative fantasy. Rather, it is an archaic folly of humankind’s desire to play god, borne out of decades of moonshot thinking that has retrospectively, in the grand context of things, floundered amidst skepticism and even ridicule. To even begin discussing where we are currently headed with [deep learning (2015)](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf), we should revisit its trajectory to explore themes in its modern development that mirror its historical vicissitudes.

While the modern-day recipes for AI systems are quite textbook and taught as such, the first notions of artificial neural learning were proposed in [the seminal paper of AI (1943)](https://link.springer.com/article/10.1007/bf02478259) and were brought to life in the form of the [multilayer perceptron (1957)](https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf). Over the next 20 years, institutions poured millions of dollars into AI research, until unmet promises for intelligent and military capabilities led to its defunding in 1974. This (first) AI winter ended in the early 1980s, when John Hopfield introduced [Hopfield neural networks (1982)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238/pdf/pnas00447-0135.pdf), inspired by the [Ising model of neural networks (1974)](http://wexler.free.fr/library/files/little%20(1974)%20the%20existence%20of%20persistent%20states%20in%20the%20brain.pdf). Hopfield networks were the first recurrent neural networks of sorts: content-addressable “associative” memory systems designed after human memory, whose decisions flows between its nodes. Concurrently, Geoffrey Hinton and David Rumelhart popularized [backpropagation (1986)](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf), expanding on Seppo Linnainmaa’s [reverse mode automatic differentiation (1970)](https://people.idsia.ch/~juergen/linnainmaa1970thesis.pdf) to allow for automatic end-to-end training of neural networks. These improvements in architecture and optimization helped revive connectionist ideology (i.e. neural networks with many layers).

The field of AI then took off for a second time, until the dwindling market of specialized AI hardware stunted neural network improvements, leading to some 300 AI companies going bankrupt by 1993. It turned out that computational power was the limiting reagent in AI chemistry. This (second) AI winter came to an end as [Moore’s Law](https://en.wikipedia.org/wiki/Moore%27s_law) took its stride: in 1997, IBM Watson’s [Deep Blue (2002)](https://core.ac.uk/download/pdf/82416379.pdf) chess engine beat reigning champion Garry Kasparov and was measured to compute at 10 million times the speed of digital computers back in 1951. When Nvidia pioneered the first GPU in 1999, it became self-evident that the fundamental problem of raw computing power would slowly give way. This time around, however, the promises of AI were wary (especially by those in the business world). Over the next decade, researchers carefully tackled concrete subfields focused on particular problems or approaches instead of “solving” artificial intelligence.

In 2012, modern (as still recognizable in 2022) deep learning was born, marking the rising action of the plot to create intelligence (and perhaps our own ex machina). The pioneering model, [AlexNet (2012)](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html), was not the first nor the fastest convolutional neural network, but won the ImageNet Challenge (ILSVRC) with unprecedented accuracy. It combined decades of research (signal processing, GPU-parallelization, backpropagation) into arguably the first proof-of-concept that deep learning could actually work. Decades of work had led up to this singularity, which then irreversibly opened the floodgates to deep learning research. Over the next few years, the craze to concoct the best neural networks led to ever-complex neural network innovations, and the term “deep” came to mean “thousand-layer” instead of “single-digit-layer.” Though deep neural networks achieved human parity across a variety of settings ([game-playing (2015)](https://www.nature.com/articles/nature14236), [image classification (2015)](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf), [speech recognition (2016)](https://arxiv.org/pdf/1610.05256.pdf)), they were still far from intelligent. They surpassed human performance on benchmarks, but were brittle to out-of-distribution data points and made blatant errors on trivially simple tasks, suggesting a fundamental lack of reasoning and continual learning that we look to as the hallmark of intelligence.

The incumbency of deep learning methodology in the early 2010s was supercharged in 2017 with the advent of a new but reminiscent architecture: the [Transformer (2017)](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (named quite adequately in retrospect). The Transformer, for the most part, did not make prior research obsolete, but instead unified many probabilistic, information theoretic, and graphical understandings of deep learning. It revolutionized neural networks, allowing them to comprehensively uncover the rich structure underlying data, leading to performance gains unequivocally. Simultaneously, there arose a movement to address the robustness of neural networks. As humans, we aggregate and draw upon information to excel at new tasks throughout our lives. Until then, there had been a focus on supervised learning for specific (discriminative) tasks, which was inherently constrained by the quantity and quality of labeled data. With the advent of a capable and scalable architecture, the focus shifted heavily to self-supervised methods (i.e. contrastive learning, generative modeling) as a way of distilling complex world information into the network prior to performing a specific task ([natural language (2018)](https://arxiv.org/pdf/1810.04805.pdf), [speech (2019)](https://arxiv.org/pdf/1904.05862.pdf), [vision (2020)](https://arxiv.org/pdf/2002.05709.pdf)). As intuition may suggest, exposing networks to vast amounts of data during pretraining improves its world knowledge and thereby its ability to perform well in downstream applications. Capturing large quantities of information requires large representative capacity, and so today in 2022, we have begun to see unprecedented phenomena emerge from neural networks by allowing them to learn patterns in data at colossal scale.

Through the innovations 2012 and 2017 and 2022, it becomes clear that given appropriate energy requirements (hardware), the symbiosis of brain (architecture) and learning (backpropagation) gives way to systems whose intelligent capabilities arise from their own instantiations. From this, we come to the interesting conclusion that intelligence is not something that we create, but rather a phenomenon exhibited by a sufficiently complex system that compresses and regurgitates world experience. As architects of AI, our duty is then to define a system whose parameters are sufficient to derive and evolve intelligence, as opposed to directly constructing intelligent ourselves.
