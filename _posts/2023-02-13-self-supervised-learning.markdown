---
layout: post
title:  "self-supervised learning"
date:   2023-02-13 00:00:00 -0400
categories: ai
---

This section aims to examine the now-prevalent pillar of deep learning: self-supervised learning. Traditionally, the field had been split into two - discriminative versus generative - though a more adequate dichotomy would be representation learning and generative modeling. Self-supervised learning reflects most closely how we, as humans, accumulate and aggregate knowledge and intuition throughout our lives. Representation learning and generative modeling are not mutually exclusive (i.e. autoregressive encoder-decoder architectures), but go after different properties.

In this section, I use “supervised learning” to denote learning with labeled examples separately from “self-supervised learning” with unlabeled examples, which is technically still “supervised learning.”

### representation learning

In the last 5 years, there has been a migration to work on learning general neural networks in and across domains, as opposed to solving individual tasks within domains. We see this in natural language ([ELMo (2018)](https://aclanthology.org/N18-1202/), [BERT (2018)](https://arxiv.org/pdf/1810.04805.pdf), [GPT (2018)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [GPT-2 (2018)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [XLNet (2019)](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf), [T5 (2019)](https://dl.acm.org/doi/pdf/10.5555/3455716.3455856), [GPT-3 (2020)](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf), [PaLM (2022)](https://arxiv.org/pdf/2204.02311.pdf)), computer vision ([VQ-VAE (2017)](https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf), [CPC (2018)](https://arxiv.org/pdf/1807.03748.pdf), [BigBiGAN (2019)](https://proceedings.neurips.cc/paper/2019/file/18cdf49ea54eec029238fcc95f76ce41-Paper.pdf), [SimCLR (2020)](https://arxiv.org/pdf/2002.05709.pdf), [SimCLR-v2 (2020)](https://proceedings.neurips.cc/paper/2020/file/fcbc95ccdd551da181207c0c1400c655-Paper.pdf), [BYOL (2020)](https://papers.nips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf), [MoCo (2020)](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf), [SwAV (2021)](https://arxiv.org/pdf/2006.09882.pdf)), and speech ([VQ-VAE (2017)](https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf), [CPC (2018)](https://arxiv.org/pdf/1807.03748.pdf), [wav2vec (2019)](https://arxiv.org/pdf/1904.05862.pdf), [wav2vec 2.0 (2020)](https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf), [HuBERT (2021)](https://arxiv.org/pdf/2106.07447.pdf)). Many of these methods contain generative components, since predicting missing or corrupted inputs is a useful way of encouraging the neural network to learn robust features in the process.

Self-supervised learning bypasses the primary problem of supervised learning pipelines: lots of high-quality labels are expensive to obtain. From a representation learning perspective, we want our deep learning systems to be robust to potential distributional gaps in real-world cases. By construction, the versatility of neural features is upper-bounded by the finiteness of its training dataset, which historically has been constrained by the cost of high-quality annotations. A dataset is a mode-coverage of the underlying data distribution, so accommodation for additional modes requires expansion of the dataset. Therefore, supervised learning suffers not only from needing labels, but also from a priori knowledge of what modes the dataset should cover. But what fundamentally breaks this perspective is that this is not how we learn as humans – at least not for most of our conscious lives. For example, to perform Q&A on the SATs, we do not train ourselves to learn English from scratch on all past SATs, but instead learn English more generally then hone our reading comprehension skills to prepare. Furthermore, we can see that reading and writing of the language improve each other, despite being different in nature. In the limit, we can conclude that the best way to improve on SATs would be to read the entire internet and then do a lot of practice SAT tests. This approach is simpler and generalizes to other skills too, so [Occam’s Razor](https://en.wikipedia.org/wiki/Occam%27s_razor) tells us that this is a better solution than becoming exclusively specialized at SATs.

Self-supervised pretraining methods have reached parity with supervised learning methods, and consistently improve with model and dataset size. After all, representation learning boils down to having the capacity to distill as much of the world’s modes into a finite set of features. We do need to handle “self-supervised learning at scale” carefully. Though reading the entire internet would indeed improve our reading comprehension skills, we can sift through our corpora and realize that shitposts on Twitter wouldn’t exactly contribute to our SAT performance. The same is true for self-supervised pretraining in practice. Numerous iterations over large-scale natural language corpora ([The Pile (2020)](https://pile.eleuther.ai/paper.pdf), [C4 (2020)](https://dl.acm.org/doi/abs/10.5555/3455716.3455856)) aim to improve dataset quality. [Codex (2021)](https://arxiv.org/pdf/2107.03374.pdf) was, unsurprisingly, trained on GitHub repositories (after filtering out short, auto-generated, non-alphanumeric files) to be able to write code well. Recently, domain-specific foundation models ([Replit (2022)](https://blog.replit.com/llms) for code data, [BloombergGPT (2023)](https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/) for financial data, [Med-PaLM 2 (2023)](https://blog.google/technology/health/ai-llm-medpalm-research-thecheckup/) for healthcare data) have been shown to require much less scale to perform well on their intended tasks. More general work on dialogue systems ([InstructGPT (2022)](https://openreview.net/pdf?id=TG8KACxEON), [ChatGPT (2022)](https://openai.com/blog/chatgpt/)) train on a mix of high-quality text and code data, with additional signal from human feedback to improve alignment. Contrastive learning in computer vision must take into account various invariances (i.e. rotational invariance) for the resulting models to pick these up. In a way, self-supervised methods still are constrained by their training settings, but not by hand-labeled examples. We should then add an asterisk to the Occam’s Razor hypothesis of the previous paragraph: we should train generally rather than specifically, but we can sort of define what “general” means in the specific setting we care about.

### generative modeling

Generative modeling has been more of a side-avenue in mainstream deep learning curriculum, probably due a higher bar of performance for it to be practical. Historically speaking, deep learning systems that perceive stimuli in actionable ways have been easier to create than the reverse: capturing that stimuli and regurgitating it at high degrees of realism. Whereas representation learning seeks to produce general features from distributions, generative modeling focuses on creating realistic samples that come from those distributions. Here, we can reference Einstein’s famous saying that “if you can't explain it (simply), you don't understand it well enough.” There have been many classes of generative models over the past decade ([variational autoencoders (2013)](https://openreview.net/forum?id=33X9fd2-9FyZd), [generative adversarial networks (2014)](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf),  [autoregressive models (2016)](https://arxiv.org/pdf/1601.06759.pdf), [normalizing flows (2018)](https://arxiv.org/pdf/1807.03039.pdf), [score-based models (2019)](https://arxiv.org/abs/2011.13456) and [diffusion models (2020)](https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf), [consistency models (2023)](https://arxiv.org/abs/2303.01469)), though it has only been in the past couple of years generative models have reached unprecedented quality.

Generative models seek to mimic the data distribution that they are trained on. The notion of overfitting sort of breaks down here – they are overfit over a specific distribution p(x) and should not be expected to generate samples that are out-of-distribution, but well-defined support over p(x) means that it will be able to interpolate between modes and generate novel samples. For example, Stable Diffusion was trained on a lot of images. Many of them contain the color green, and there were probably many with fire hydrants. However, even though “green fire hydrants” do not exist, it is able to interpolate between the modes of “green” and “fire hydrant” to fill in this hole in the empirical distribution. I prefer to describe this phenomenon as more common sense than hallucination.

Since real-world probability distributions are intractable to represent explicitly, generative models are trained to map a simpler distribution (i.e. standard Gaussian) to the empirical distribution, usually by maximizing the log-likelihood of the samples. In continuous domains this is a maximization of the variational lower bound, and in discrete domains this is a minimization of the cross-entropy between learned and empirical distributions. Generative models in both fields have consolidated in the past 3-5 years to the following two frameworks: diffusion models and autoregressive Transformers (this is just a rough categorization whose properties are not mutually exclusive). I detail them both in their own paragraphs below.

Originally applied to class-conditional image synthesis, diffusion models have excelled at text-to-image synthesis. It turns out that a large contributor to text-to-image quality is the text embedding quality – so much so that large marginal gains come from larger/better text encoders than diffusion capabilities. With the success of text-to-image models ([DALL-E (2021)](https://arxiv.org/pdf/2102.12092.pdf), [ERNIE-ViLG (2021)](https://arxiv.org/pdf/2112.15283.pdf), [DALL-E-2 (2022)](https://cdn.openai.com/papers/dall-e-2.pdf), [Imagen (2022)](https://arxiv.org/pdf/2205.11487.pdf), [GLIDE (2022)](https://arxiv.org/pdf/2112.10741.pdf), [Stable Diffusion (2022)](https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf), [Parti (2022)](https://openreview.net/pdf?id=AFDcYJKhND), [eDiff-I (2022)](https://arxiv.org/pdf/2211.01324.pdf), [ERNIE-ViLG 2.0 (2022)](https://arxiv.org/pdf/2210.15257.pdf)), we begin to observe trends that are unparalleled in earlier generative models, with the most relevant being scale. Generative models require large capacity to successfully cover all modalities of its empirical distribution, as observed in improved performances due to greater depth, width, and computational mechanisms as shown in [BigGAN (2018)](https://openreview.net/pdf?id=B1xsqj09Fm). Datasets in the text-to-image realm are also richer than ImageNet, containing textual-visual associative information across a variety of natural image classes. As a result, the smallest text-to-image models (1.4B parameters) are at least the size of the largest generative models from a few years ago (BigGAN, at 340M parameters). In this super large neural regime, new properties emerge: the Parti authors scale autoregressive text-to-image models through multiple magnitudes of parameters and show improved alignment with text and better understanding of composition as a result.

Transformers were originally designed for machine translation and sequence-to-sequence tasks, but its main contribution has withstood the test of time. They rely on explicit attention mechanisms to aggregate dependencies across long sequences, and feed-forward layers to fuse them together. It amends the catastrophic memory bottleneck of RNNs at the quadratic cost of modeling pairwise interactions explicitly. Since its invention in 2017, it has proven to be the architectural pillar of modern deep neural networks in all domains of research. The attention mechanism allows the Transformer to model all complex dependencies in inputs, and scale arbitrarily without plateauing in performance.

If it is not clear at this point already, the reason these frameworks have observed long-term success lies in their ability to scale. Larger datasets cover more modes of data. Larger neural networks compress this data better. Though they require larger models to represent this coverage, the resulting neural networks end up distilling evermore complex concepts of the world to stunning quality. However, as well as diffusion and Transformers scale, there are interesting observations to be made at the opposite end of the spectrum: people have observed that [small diffusion models fail to learn anything](https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/) and [small Transformers perform worse than RNNs](https://www.reddit.com/r/MachineLearning/comments/dlhcub/d_are_small_transformers_better_than_small_lstms/). Through these insights, we can maybe understand how the timing of these innovations is probably not coincidental, and that there are entangled minimum requirements that exist for truly excellent deep learning.

### scaling laws

Scale refers to the size of neural networks (in number of participating neurons) and their training corpora (in number of domain-specific atomic units, i.e. 1 document for GPT-3). Somewhat ironically, scale is a brute-force method from which the very foundations of biological organisms are constructed, so it perhaps should not be a surprising phenomenon in artificial neurons. Unlike the deep learning innovations of the pre-Transformer era, scale does not attempt to augment neural capacity by introducing some clever new parameterization, or some sexy new trick. Under some assumptions (i.e. neural architecture sufficiency), it claims that more is not only better — but better with unforeseen properties. GPT did not just improve unilaterally across NLP benchmarks from GPT-2 to GPT-3, but learned to recognize instruction and when it should be a participant in its own conversation. This awareness suggests a greater understanding of natural language. Similar trends have been observed in recent text-to-image findings, in which scaling neural capacity through orders of magnitudes leads to models that understand object composition, photorealistic lighting, and coherence of text in its synthetic images. We hope to measure these qualities of intelligence with heuristics and benchmarks, but the qualitative emergence of intelligence comes from self-organization at scale.

Naturally, we now ask ourselves what happens to this phenomenon in the limit and, as a result, stumble upon a set of governances imposed by the universe. If we follow these [scaling laws](https://en.wikipedia.org/wiki/Neural_scaling_law), we hastily conclude that AGI lurks at some asymptotic point, but we know that this hypothesis has to be incomplete. Existing investigations of scaling laws plot pareto curves constrained on 3 degrees of freedom (dataset scale, compute scale, model scale), disregarding the effects of [data pruning (2022)](https://openreview.net/pdf?id=UmvSlP-PyV) and [multimodality (2023)](https://arxiv.org/pdf/2301.03728.pdf). This incompleteness has seen [disproof](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models) of the [original scaling laws paper (2020)](https://arxiv.org/pdf/2001.08361.pdf). Furthermore, true intelligence requires a more conscious framework than automatic world compression via deep learning. It requires that, at any point in its existence, it be able to recognize when its own knowledge is insufficient, step back from its continual learning process, and piece together atomic units of intuition to adapt.

We are now approaching the long tail of scaling laws, which will transition us out of the current paradigm of deep learning. The close of this chapter might resemble the introduction of the Transformer, which did not disprove anything but rather expanded what we knew and ushered in the new paradigm of scale. The next paradigm won’t make current deep learning at scale obsolete; it will simply show scale to be incomplete, a symptom of our ever-growing understanding of the universe ([Newtonian mechanics turns out to be a rough approximation](https://en.wikipedia.org/wiki/Classical_mechanics#Limits_of_validity) of the more complex frameworks of special relativity and quantum mechanics). Ultimately, scale cannot outgrow its architectural confines and requires a new framework of conscious reasoning that taps into subconscious learning (deep learning) to adapt to new problems.

From this perspective, Einstein had gotten it completely wrong. “Insanity is doing the same thing over and over and expecting different results” could not be further from the truth. Rote repetition is the source of all growth in cellular automata that allows complex mechanisms to figure out the optimal configuration of their subcomponents.
