---
layout: post
title:  "reinforcement learning"
date:   2023-05-22 00:00:00 -0400
categories: ai
---

We begin by summarizing intelligence as the ability to adapt to new contexts (this is one out of many angles). Intelligence manifests itself in social situations (i.e. emotional intelligence), creativity, and critical thinking problems (i.e. cognitive intelligence). Though it is intangible, it is baked into the trail of data that we leave behind. At this point in the essay, it should be clear that large deep neural networks have become so proficient that they can mimic our data (and intelligence) distributions. As a result, they also pick up on higher-order notions of intelligence – emotion, originality, etc – that are embedded within our data footprints. Artificial intelligence covers our own distributions so well that, in conjunction with its ability to compute, it interpolates our sparse data footprints to find better solutions. In many generative and game-playing domains, we find artificially intelligent agents so much better – often by orders of magnitude – that we use them as oracles to evaluate our own competence.

The embedding of humanity into these generative models may come as shocking to many – how is it possible that we can not only mimic Drake’s voice, but also sync them to lyrics that are written in his style of rap? The information theoretic way of understanding our human footprint is that all the data we generate is information, whose unit of measurement is the bit. The bit is the simplest unit of measurement that we can string together in binary numbers to represent exponentially more things. Technically, we need as many bits as there are in the data to perfectly represent the data (i.e. memorization) but this quickly becomes unscalable. The manifold hypothesis of deep learning presumes that all these bits of information in the data can be explained by a smaller number of bits – or that the seemingly infiniteness of data actually lies in a lower dimension. Think about it this way – the number of possible sentences in the English language is infinite despite there being a finite number of words, so by learning even a few thousand words we are able to generate infinitely many bits of data.

With such powerful compression of humanity’s bits into a finite neural network, the competence of generative AI becomes quite subjective. Over many interactions with the dialogue agent, it is hard to say which response to “how does deep learning work” is more correct. Assuming factual correctness, the answer to this question is not yes or no, it is a preference. Unsurprisingly, there are ways of speaking/writing that we prefer (i.e. more pathos, less rigid), and the competent neural network should be aligned with our preferences. However, unlike accuracy of predicting the next word, preference isn’t quite as easily defined. Instead, we can treat the model as an agent whose actions generate words and whose long-term goal is to maximize human preference. We can then view this as a [reinforcement learning problem from human feedback](https://openai.com/research/learning-from-human-preferences), where the reward encodes abstract notions of human preference.

### reinforcement learning

Concurrent to the massive consolidation around (self-)supervised deep learning is reinforcement learning, whose origins trace back at least half a century. The motivation of reinforcement learning is to mirror how humans and autonomous agents interact with the real world – at each point in time, an agent has the ability to take an action / make a decision. This decision then has repercussions in the world environment, whose state is modified accordingly and exposed again to the agent at the next decision-making point in time. The idea here is that as we learn to navigate our world, we go through a process of trial and error, through which we learn behavior that maximizes some objective in the environment around us. Unlike the framework of self-supervised learning in which a prediction (which we can interpret as a single action) is self-contained, in reinforcement learning these predictions over time build up a Markov chain of states that have some effect in the long term. Therefore, it is no longer a simple prediction that can be measured by one-shot accuracy, but a chain of predictions that is measured by performance on some downstream goal.

Naturally, we would define such a goal as a reward that we want to maximize. The idea of reward is abstract and arbitrary – in chess, perhaps we would ascribe +1 to a win and -1 to a loss, and allow the agent to autonomously figure out the best sequence of actions over 50 moves that would maximize reward (and therefore the chances of success). Compression of the environment as inputs to the agent at each decision-making timestep is done via a deep neural network, which typically outputs some distribution over the set of possible actions, and is learned by backpropagating the reward signal from the environment (i.e. when we learn to ride a bike and fall off, we subconsciously learn to not repeat this sequence of actions). To remove dependence on how the world environment evolves and explore subspaces that exist outside of the current policy, we do Q-learning, which learns to evaluate the “current reward” of world states and actions. By learning this with a deep neural network, we can hypothesize which action will lead to better reward, which turns out to be [the pivotal innovation in solving game-play](https://www.nature.com/articles/nature14236). The same neural architectural improvements that supercharged the success of deep learning in the late 2010s also improved the capacity of these neural value functions which have led to the stunning solutions in [chess (2018)](https://arxiv.org/abs/1712.01815), [protein-folding (2021)](https://www.deepmind.com/publications/highly-accurate-protein-structure-prediction-with-alphafold), and [multi-modality (2022)](https://openreview.net/pdf?id=1ikK0kHjvj).

Until recently, reinforcement learning agents were able to autonomously learn solutions to maximize reward functions for specific tasks, but non-transferrable skills are impractical and also a severe shortcoming of intelligence. We see that many of these deep neural modules (i.e. Q-network) or internal transition models (i.e. [Dreamer (2019)](https://arxiv.org/abs/1912.01603)) are learned from scratch for specific tasks, inducing longer training times due to the concurrent optimization of world understanding and decision-making. As a result, training variance, sample efficiency, and good exploration have traditionally been the major pitfalls of reinforcement learning algorithms from scratch, but the latest trends in foundation models may disentangle much of the reinforcement learning process.

While jury was out when it comes to the implementation of autonomously learning and acting agents, it may be slowly concurring on the codependency of learning and acting. The grandfather of reinforcement learning, Richard Sutton, famously proposes [a cellular automaton that lays out the fundamental building blocks of AGI (2022)](https://arxiv.org/pdf/2208.11173.pdf): perception model, transition model, reward (critic) function, and policy (actor). Recently, we've seen large autoregressive language models demonstrate properties of reasoning via natural language, but are still susceptible to poor lines of reasoning. From the perspective of [dual process theory](https://en.wikipedia.org/wiki/Dual_process_theory), this model breaks down and is insufficient. Recent work has shown that introducing the ability to [chain thoughts together sequentially (2023)](https://arxiv.org/pdf/2201.11903.pdf) or [backtracking unwanted rollouts (2023)](https://arxiv.org/pdf/2306.05426.pdf) provides some higher level of conscious reasoning over the unconscious reasoning learned by the neural network trained via self-supervised learning. Even more recently, Deepmind researchers marry language model reasoning with reinforcement learning exploration in [trees of thought (2023)](https://arxiv.org/pdf/2305.10601.pdf), in which the language model is a subconscious value function that is free to explore the state space grown by [Monte Carlo tree search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search). The subconscious process of compression via self-supervised learning and the conscious process of direction via reinforcement learning require each other to function symbiotically.

### modularity

Perhaps another interesting way to think about saturating scaling laws and how they may fit into the larger puzzle is dimensionality. Suppose the problem we want to solve lies in the 3D subspace. If we fix some parameters, we might find that this constrained problem lies on some 2D subspace. Though we can optimize on this plane, the true solution requires unfixing some of the parameters such that we can traverse a larger subspace to find a global optimum. The analogy here is that current investigations of scaling laws only operate on some unimodal static dataset. We find solutions here that are too constrained to solve the higher dimensional problem of intelligence. From this perspective, scale is inadequate since it alone cannot break the confines of its own dimensionality. Rather, in the same way that points create length, lengths create area, and areas create volume, scale provides building blocks that can be glued together into higher dimensionality. Let’s roll back this discussion to something more concrete.

In my first AI class back in 2018, I found at least 50% of my code and debugging to be boilerplate: training loops, logging, checkpointing, and rudimentary modules that can all be taken care of in a couple imports today (at some point in a more distant past, researchers had to write their own deep learning libraries). Since then, we have seen the evolution and consolidation of deep learning libraries like PyTorch and JAX, on top of which we have witnessed the emergence of entire ecosystems, like HuggingFace and Pytorch Lightning, as well as open-source contributions of research. As a result, we’ve seen increasing modularity from layers to blocks to models to entire training procedures including datasets, optimization, and parallelism; writing distributed data parallel code in native PyTorch is almost unheard of for the average researcher today. Recently, modularization has begun to climb to higher levels with the success of large (and largely irreproducible) foundation models as APIs.

The capabilities of ChatGPT, Stable Diffusion, and other foundation models now allow “the model” to be abstracted away such that products can import them as modules. In fact, this latest layer of modularization has already arrived. In 2023, LLMs hit the minimal threshold of proficiency in natural language understanding. As such, they can be queried and prompted recursively since they can understand their own outputs as context. Already, open-source libraries have popped up to exploit this property: [LangChain](https://github.com/hwchase17/langchain) for chaining queries to LLMs as compressive search engines, [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT) for recursion into the LLM to achieve goals set by natural language queries, and [babyagi](https://github.com/yoheinakajima/babyagi) for recursively executing tasks as subtasks. Frankly, it is insane that intelligence via natural language can now be wrapped up in a Python package and called as an endpoint.

Modularization happens immediately at a code level, so it may be tempting to chalk this up to improvements in code development. Though this is not untrue, modularization can be seen more as a property of technological innovation (the steam engine, cars, electricity, etc only had to be invented once) and the means through which nature builds complexity. Deep learning systems are constructed from blocks of increasing levels of abstraction: individual layers create blocks, simple blocks create mechanisms, complex mechanisms create models, and models should create intelligence if they are interfaced correctly. Of course, this is a grand simplification of deep learning systems, but modularization always ends up being the entropy-minimizing organization of information.
