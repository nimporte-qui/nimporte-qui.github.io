---
layout: post
title:  "reinforcement learning"
date:   2023-05-22 00:00:00 -0400
categories: ai
---

### reinforcement learning

Concurrent to the massive consolidation around (self-)supervised deep learning is reinforcement learning, whose origins trace back at least half a century. The motivation of reinforcement learning is to mirror how humans and autonomous agents interact with the real world – at each point in time, an agent has the ability to take an action / make a decision. This decision then has repercussions in the world environment, whose state is modified accordingly and exposed again to the agent at the next decision-making point in time. The idea here is that as we learn to navigate our world, we go through a process of trial and error, through which we learn behavior that maximizes some objective in the environment around us. Unlike the framework of self-supervised learning in which a prediction (which we can interpret as a single action) is self-contained, in reinforcement learning these predictions over time build up a Markov chain of states that have some effect in the long term. Therefore, it is no longer a simple prediction that can be measured by one-shot accuracy, but a chain of predictions that is measured by performance on some downstream goal.

Naturally, we would define such a goal as a reward that we want to maximize. The idea of reward is abstract and arbitrary – in chess, perhaps we would ascribe +1 to a win and -1 to a loss, and allow the agent to autonomously figure out the best sequence of actions over 50 moves that would maximize reward (and therefore the chances of success). Compression of the environment as inputs to the agent at each decision-making timestep is done via a deep neural network, which typically outputs some distribution over the set of possible actions, and is learned by backpropagating the reward signal from the environment (i.e. when we learn to ride a bike and fall off, we subconsciously learn to not repeat this sequence of actions). To remove dependence on how the world environment evolves and explore subspaces that exist outside of the current policy, we do Q-learning, which learns to evaluate the “current reward” of world states and actions. By learning this with a deep neural network, we can hypothesize which action will lead to better reward, which turns out to be [the pivotal innovation in solving game-play](https://www.nature.com/articles/nature14236). The same neural architectural improvements that supercharged the success of deep learning in the late 2010s also improved the capacity of these neural value functions which have led to the stunning solutions in [chess (2018)](https://arxiv.org/abs/1712.01815), [protein-folding (2021)](https://www.deepmind.com/publications/highly-accurate-protein-structure-prediction-with-alphafold), and [multi-modality (2022)](https://openreview.net/pdf?id=1ikK0kHjvj).

Until recently, reinforcement learning agents were able to autonomously learn solutions to maximize reward functions for specific tasks, but non-transferrable skills are impractical and also a severe shortcoming of intelligence. We see that many of these deep neural modules (i.e. Q-network) or internal transition models (i.e. [Dreamer (2019)](https://arxiv.org/abs/1912.01603)) are learned from scratch for specific tasks, inducing longer training times due to the concurrent optimization of world understanding and decision-making. As a result, training variance, sample efficiency, and good exploration have traditionally been the major pitfalls of reinforcement learning algorithms from scratch, but the latest trends in foundation models may disentangle much of the reinforcement learning process.

While jury was out when it comes to the implementation of autonomously learning and acting agents, it may be slowly concurring on the codependency of learning and acting. The grandfather of reinforcement learning, Richard Sutton, famously proposes [a cellular automaton that lays out the fundamental building blocks of AGI (2022)](https://arxiv.org/pdf/2208.11173.pdf): perception model, transition model, reward (critic) function, and policy (actor). Recently, we've seen large autoregressive language models demonstrate properties of reasoning via natural language, but are still susceptible to poor lines of reasoning. From the perspective of [dual process theory](https://en.wikipedia.org/wiki/Dual_process_theory), this model breaks down and is insufficient. Recent work has shown that introducing the ability to [chain thoughts together sequentially (2023)](https://arxiv.org/pdf/2201.11903.pdf) or [backtracking unwanted rollouts (2023)](https://arxiv.org/pdf/2306.05426.pdf) provides some higher level of conscious reasoning over the unconscious reasoning learned by the neural network trained via self-supervised learning. Even more recently, Deepmind researchers marry language model reasoning with reinforcement learning exploration in [trees of thought (2023)](https://arxiv.org/pdf/2305.10601.pdf), in which the language model is a subconscious value function that is free to explore the state space grown by [Monte Carlo tree search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search). The subconscious process of compression via self-supervised learning and the conscious process of direction via reinforcement learning require each other to function symbiotically.